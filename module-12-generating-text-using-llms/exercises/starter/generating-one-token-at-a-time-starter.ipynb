{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise: Generating one token at a time\n",
        "\n",
        "In this exercise, we will get to understand how an LLM generates text--one token at a time, using the previous tokens to predict the following ones.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1. Load a tokenizer and a model\n",
        "\n",
        "First we load a tokenizer and a model from HuggingFace's transformers library. A tokenizer is a function that splits a string into a list of numbers that the model can understand.\n",
        "\n",
        "In this exercise, all the code will be written for you. All you need to do is follow along!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/xina/dev/Spam_Classifier/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Student Task: Load the tokenizer and model for `gpt2` from HuggingFace\n",
        "# TODO: Replace the ********** with the correct code\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# To load a pretrained model and a tokenizer using HuggingFace, we only need two lines of code!\n",
        "# tokenizer = **********\n",
        "# model = **********\n",
        "model_name= \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  52,   67, 4355,  318,  262, 1266, 1295,  284, 2193,  546, 1152,  876]])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We create a partial sentence and tokenize it.\n",
        "# No changes needed here\n",
        "\n",
        "text = \"Udacity is the best place to learn about generative\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# Show the tokens as numbers, i.e. \"input_ids\"\n",
        "inputs[\"input_ids\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(52) U\n",
            "tensor(67) d\n",
            "tensor(4355) acity\n",
            "tensor(318)  is\n",
            "tensor(262)  the\n",
            "tensor(1266)  best\n",
            "tensor(1295)  place\n",
            "tensor(284)  to\n",
            "tensor(2193)  learn\n",
            "tensor(546)  about\n",
            "tensor(1152)  gener\n",
            "tensor(876) ative\n"
          ]
        }
      ],
      "source": [
        "for id in inputs[\"input_ids\"][0]:\n",
        "    print(id, tokenizer.decode(id))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2. Examine the tokenization\n",
        "\n",
        "Let's explore what these tokens mean!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>token</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tensor(52)</td>\n",
              "      <td>U</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tensor(67)</td>\n",
              "      <td>d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tensor(4355)</td>\n",
              "      <td>acity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>tensor(318)</td>\n",
              "      <td>is</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tensor(262)</td>\n",
              "      <td>the</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>tensor(1266)</td>\n",
              "      <td>best</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>tensor(1295)</td>\n",
              "      <td>place</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>tensor(284)</td>\n",
              "      <td>to</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>tensor(2193)</td>\n",
              "      <td>learn</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>tensor(546)</td>\n",
              "      <td>about</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>tensor(1152)</td>\n",
              "      <td>gener</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>tensor(876)</td>\n",
              "      <td>ative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              id   token\n",
              "0     tensor(52)       U\n",
              "1     tensor(67)       d\n",
              "2   tensor(4355)   acity\n",
              "3    tensor(318)      is\n",
              "4    tensor(262)     the\n",
              "5   tensor(1266)    best\n",
              "6   tensor(1295)   place\n",
              "7    tensor(284)      to\n",
              "8   tensor(2193)   learn\n",
              "9    tensor(546)   about\n",
              "10  tensor(1152)   gener\n",
              "11   tensor(876)   ative"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Show how the sentence is tokenized\n",
        "# No changes needed here\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def show_tokenization(inputs):\n",
        "    return pd.DataFrame(\n",
        "        [(id, tokenizer.decode(id)) for id in inputs[\"input_ids\"][0]],\n",
        "        columns=[\"id\", \"token\"],\n",
        "    )\n",
        "\n",
        "\n",
        "show_tokenization(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Subword tokenization\n",
        "\n",
        "The interesting thing is that tokens in this case are neither just letters nor just words. Sometimes shorter words are represented by a single token, but other times a single token represents a part of a word, or even a single letter. This is called subword tokenization.\n",
        "\n",
        "## Step 2. Calculate the probability of the next token\n",
        "\n",
        "Now let's use PyTorch to calculate the probability of the next token given the previous ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the probabilities for the next token for all possible choices. We show the\n",
        "# top 5 choices and the corresponding words or subwords for these tokens.\n",
        "# No changes needed here\n",
        "\n",
        "import torch\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits[:, -1, :]\n",
        "    probabilities = torch.nn.functional.softmax(logits[0], dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The line logits = model(**inputs).logits[:, -1, :] is extracting the model's predictions for the next token in text generation. Let me break it down step by step:\n",
        "\n",
        "### What the Model Outputs\n",
        "- model(**inputs): Runs the GPT-2 model on your tokenized input text\n",
        "- .logits: Gets the raw prediction scores (logits) from the model\n",
        "    - Shape: (batch_size, sequence_length, vocab_size)\n",
        "    - For each position in your input sequence, it predicts scores for every possible next token\n",
        "### The Slicing: [:, -1, :]\n",
        "- [:, -1, :] selects:\n",
        "    - : (first dimension): All batches (usually just 1 in this case)\n",
        "    - -1 (second dimension): The last position in the sequence (the most recent token)\n",
        "    - : (third dimension): All vocabulary scores (one score per possible token)\n",
        "### Why This Specific Slice?\n",
        "In autoregressive text generation, you want the model's prediction for what comes next after your current text\n",
        "[:, -1, :] gives you the logits for the next token, based on the entire input sequence\n",
        "These logits can then be converted to probabilities (via softmax) to choose the most likely next word\n",
        "### Example\n",
        "If your input is \"Hello world\" (tokenized to 2 tokens), the model outputs logits for every position:\n",
        "\n",
        "- Position 0: Predictions after \"Hello\"\n",
        "- Position 1: Predictions after \"Hello world\" ‚Üê This is what [:, -1, :] grabs\n",
        "\n",
        "This is the core of how LLMs generate text one token at a time! The model uses all previous context to predict what's most likely to come next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_logits = model(**inputs).logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 12, 50257])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_logits.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-77.0661, -75.2075, -79.4712,  ..., -79.0780, -83.3320, -76.6110],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_logits[0][-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([50257])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "probabilities.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>token</th>\n",
              "      <th>p</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8300</th>\n",
              "      <td>8300</td>\n",
              "      <td>programming</td>\n",
              "      <td>0.117578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4673</th>\n",
              "      <td>4673</td>\n",
              "      <td>learning</td>\n",
              "      <td>0.082171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8950</th>\n",
              "      <td>8950</td>\n",
              "      <td>languages</td>\n",
              "      <td>0.028905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3725</th>\n",
              "      <td>3725</td>\n",
              "      <td>knowledge</td>\n",
              "      <td>0.026193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>290</td>\n",
              "      <td>and</td>\n",
              "      <td>0.025532</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id         token         p\n",
              "8300  8300   programming  0.117578\n",
              "4673  4673      learning  0.082171\n",
              "8950  8950     languages  0.028905\n",
              "3725  3725     knowledge  0.026193\n",
              "290    290           and  0.025532"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "def show_next_token_choices(probabilities, top_n=5):\n",
        "    return pd.DataFrame(\n",
        "        [\n",
        "            (id, tokenizer.decode(id), p.item())\n",
        "            for id, p in enumerate(probabilities)\n",
        "            if p.item()\n",
        "        ],\n",
        "        columns=[\"id\", \"token\", \"p\"],\n",
        "    ).sort_values(\"p\", ascending=False)[:top_n]\n",
        "\n",
        "\n",
        "show_next_token_choices(probabilities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.DataFrame(\n",
        "        [\n",
        "            (id, tokenizer.decode(id), p.item())\n",
        "            for id, p in enumerate(probabilities)\n",
        "            if p.item()\n",
        "        ],\n",
        "        columns=[\"id\", \"token\", \"p\"],\n",
        "    ).sort_values(\"p\", ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "id             8950\n",
              "token     languages\n",
              "p          0.028905\n",
              "Name: 8950, dtype: object"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.loc[8950,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interesting! The model thinks that the most likely next word is \"programming\", followed up closely by \"learning\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Next token id: 8300\n",
            "Next token:  programming\n"
          ]
        }
      ],
      "source": [
        "# Student task: Obtain the token id for the most probable next token\n",
        "# TODO: Replace the ********** with the correct code\n",
        "\n",
        "# Get the id of the most probable next token, i.e. the index of the highest value in `probabilities`\n",
        "# Hint: torch.argmax will be useful\n",
        "# Hint: `.item()` converts a single-value tensor to a standard Python number\n",
        "# next_token_id = **********\n",
        "next_token_id = torch.argmax(probabilities).item()\n",
        "\n",
        "\n",
        "print(f\"Next token id: {next_token_id}\")\n",
        "print(f\"Next token: {tokenizer.decode(next_token_id)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Udacity is the best place to learn about generative programming'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We append the most likely token to the text.\n",
        "# No changes needed here\n",
        "\n",
        "new_text = text + tokenizer.decode(8300)\n",
        "new_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3. Generate some more tokens\n",
        "\n",
        "The following cell will take `text`, show the most probable tokens to follow, and append the most likely token to text. Run the cell over and over to see it in action!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Udacity is the best place to learn about generative programming\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**Next token probabilities:**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>token</th>\n",
              "      <th>p</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8300</th>\n",
              "      <td>8300</td>\n",
              "      <td>programming</td>\n",
              "      <td>0.117578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4673</th>\n",
              "      <td>4673</td>\n",
              "      <td>learning</td>\n",
              "      <td>0.082171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8950</th>\n",
              "      <td>8950</td>\n",
              "      <td>languages</td>\n",
              "      <td>0.028905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3725</th>\n",
              "      <td>3725</td>\n",
              "      <td>knowledge</td>\n",
              "      <td>0.026193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>290</td>\n",
              "      <td>and</td>\n",
              "      <td>0.025532</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id         token         p\n",
              "8300  8300   programming  0.117578\n",
              "4673  4673      learning  0.082171\n",
              "8950  8950     languages  0.028905\n",
              "3725  3725     knowledge  0.026193\n",
              "290    290           and  0.025532"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Press ctrl + enter to run this cell again and again to see how the text is generated.\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# Show the text\n",
        "print(new_text)\n",
        "\n",
        "# Convert to tokens\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# Calculate the probabilities for the next token and show the top 5 choices\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits[:, -1, :]\n",
        "    probabilities = torch.nn.functional.softmax(logits[0], dim=-1)\n",
        "\n",
        "display(Markdown(\"**Next token probabilities:**\"))\n",
        "display(show_next_token_choices(probabilities))\n",
        "\n",
        "# Choose the most likely token id and add it to the text\n",
        "next_token_id = torch.argmax(probabilities).item()\n",
        "new_text = new_text + tokenizer.decode(next_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4. Use the `generate` method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Student Task: Use the model.generate method to generate lots of text quickly\n",
        "# TODO: Replace the ********** with the correct code\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# Start with some text and tokenize it\n",
        "text = \"Once upon a time, generative models\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# Use the `generate` method to generate a max of 100 tokens\n",
        "# See: https://docs.pytorch.org/torchtune/0.3/generated/torchtune.generation.generate.html\n",
        "# output = **********\n",
        "output= model.generate(**inputs, max_new_tokens=100)\n",
        "\n",
        "# Show the generated text\n",
        "display(Markdown(tokenizer.decode(output[0])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### That's interesting...\n",
        "\n",
        "You'll notice that GPT-2 is not nearly as sophisticated as later models like GPT-4/5. It often repeats itself and doesn't always make much sense. But it's still pretty impressive that it can generate text that looks like English.\n",
        "\n",
        "## Congrats for completing the exercise! üéâ\n",
        "\n",
        "Give yourself a hand. And please take a break if you need to. We'll be here when you're refreshed and ready to learn more!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br /><br /><br /><br /><br /><br /><br /><br />"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
