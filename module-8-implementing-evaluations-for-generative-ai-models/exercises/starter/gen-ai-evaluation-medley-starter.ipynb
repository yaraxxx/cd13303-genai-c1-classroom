{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c946b9af",
      "metadata": {},
      "source": [
        "# Exercise: GenAI Evaluation Medley\n",
        "\n",
        "Let's practice exact evaluation, AI-as-judge mechanics, and benchmarking by completing small, focused coding tasks.\n",
        "\n",
        "## Outline\n",
        "\n",
        "We will cover the following evaluation techniques:\n",
        "1. Exact Match: Implement a function to compute the exact match score between predicted and reference answers.\n",
        "2. Lexical Similarity: Calculate ROUGE scores to assess the overlap between predicted and reference texts.\n",
        "3. Semantic Similarity: Use embeddings to compute cosine similarity between predicted and reference texts.\n",
        "4. Functional Correctness: Evaluate code generation by executing predicted code and comparing outputs.\n",
        "5. Pass@K: Implement the Pass@K metric.\n",
        "6. LLM-as-a-Judge or AI-as-a-Judge: Use a language model to evaluate the quality of predictions based on a rubric."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d72f89e6",
      "metadata": {},
      "source": [
        "## Setup\n",
        "Now we import standard libraries used across exercises and set basic configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b2778307",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Student Task: Set up the OpenAI API key and base URL from environment variables\n",
        "# TODO: If using Vocareum, set the API key directly in the code below\n",
        "\n",
        "import litellm\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "if os.getenv(\"OPENAI_API_KEY\"):\n",
        "    litellm.openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# If using Vocareum, you can also set your API key here directly\n",
        "# Uncomment and replace the string with your actual Vocareum API key\n",
        "# litellm.openai_key = \"voc-**********\"\n",
        "\n",
        "if (litellm.openai_key or \"\").startswith(\"voc-\"):\n",
        "    litellm.api_base = \"https://openai.vocareum.com/v1\"\n",
        "    print(\"Using Vocareum OpenAI API base URL\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cc74fd4",
      "metadata": {},
      "source": [
        "## Exact Match (EM)\n",
        "Let's compute exact-match accuracy after simple normalization (lowercase and trim)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "58b84285",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EM: 0.75\n"
          ]
        }
      ],
      "source": [
        "# Student task: Implement exact_match and compute EM\n",
        "# TODO: Complete the sections marked with **********\n",
        "\n",
        "preds = [\"Lima\", \"ayacucho\", \"Cusco\", \"Arequipa\"]\n",
        "labels = [\"lima\", \"Ayacucho\", \"Cusco\", \"Trujillo\"]\n",
        "\n",
        "\n",
        "def normalize(s: str) -> str:\n",
        "    \"\"\"Normalize the string by lowercasing and stripping whitespace.\"\"\"\n",
        "    return s.lower().strip()\n",
        "\n",
        "\n",
        "def exact_match(pred: str, label: str) -> int:\n",
        "    # return 1 if normalized strings are identical, else 0\n",
        "    # return_value = \"**********\"\n",
        "    return_value = int(normalize(pred) == normalize(label) )\n",
        "    return return_value\n",
        "\n",
        "\n",
        "em_scores = [exact_match(p, l) for p, l in zip(preds, labels)]\n",
        "em = sum(em_scores) / len(em_scores)\n",
        "print(\"EM:\", em)\n",
        "\n",
        "assert em == 0.75, (\n",
        "    f\"EM should be 0.75, but got {em}. Please check your exact_match function.\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c71e252",
      "metadata": {},
      "source": [
        "## Lexical Similarity (ROUGE)\n",
        "\n",
        "Let's compute ROUGE scores using the `evaluate` library.\n",
        "\n",
        "Read more at: https://huggingface.co/spaces/evaluate-metric/rouge/blob/main/README.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b039506b",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/xina/dev/Spam_Classifier/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'rouge1': np.float64(1.0),\n",
              " 'rouge2': np.float64(0.6),\n",
              " 'rougeL': np.float64(0.6666666666666666),\n",
              " 'rougeLsum': np.float64(0.6666666666666666)}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Student task: Compute ROUGE-L using LCS length\n",
        "# Complete the sections with **********\n",
        "\n",
        "\n",
        "# Define candidate and reference texts\n",
        "pred = \"The capital of Peru is Lima\"\n",
        "label = \"Lima is the capital of Peru\"\n",
        "\n",
        "\n",
        "# Import the evaluate library\n",
        "# **************\n",
        "from evaluate import load\n",
        "\n",
        "# Load the ROUGE metric\n",
        "# **************\n",
        "rouge = load(\"rouge\")\n",
        "\n",
        "# Compute ROUGE scores\n",
        "# **************\n",
        "results = rouge.compute(predictions = [pred],references = [label])\n",
        "\n",
        "\n",
        "assert isinstance(results, dict), (\n",
        "    f\"Results should be a dictionary, but got {type(results)}. See the evaluate library documentation for ROUGE usage.\"\n",
        ")\n",
        "keys = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
        "for key in keys:\n",
        "    assert key in results, (\n",
        "        f\"Missing key '{key}' in results. Expected keys: {keys}. See the evaluate library documentation for ROUGE usage.\"\n",
        "    )\n",
        "\n",
        "results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15822ca6",
      "metadata": {},
      "source": [
        "## Semantic Similarity using Cosine Similarity\n",
        "\n",
        "We'll use the `sentence-transformers` library to compute semantic similarity between predicted and reference sentences. The model \"all-MiniLM-L6-v2\" is a lightweight model that can run on GPUs.\n",
        "\n",
        "Read more here: https://sbert.net/docs/quickstart.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5d276d34",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# 1. Load a pretrained Sentence Transformer model\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# 2. Some example sentences\n",
        "sentences = [\n",
        "    \"Hi there!\",\n",
        "    \"This is a test sentence.\",\n",
        "]\n",
        "\n",
        "# 3. Generate embeddings\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "# 4. Verify we have 2 embeddings of dimension 384 each\n",
        "assert embeddings.shape == (2, 384)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9a78dfbf",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[ 0.06145668, -0.06237289, -0.03735716, ..., -0.00960593,\n",
              "          0.03519065, -0.01402609],\n",
              "        [-0.01852836, -0.03180065, -0.07411847, ..., -0.00197018,\n",
              "          0.01199767,  0.0113026 ],\n",
              "        [ 0.10953207, -0.02914077,  0.02478122, ..., -0.00410273,\n",
              "         -0.05964335,  0.06561526]], shape=(3, 384), dtype=float32),\n",
              " array([[ 0.06701855, -0.04063963, -0.06178873, ...,  0.01089183,\n",
              "         -0.01366579, -0.02568757],\n",
              "        [ 0.08464757,  0.00272666, -0.0645582 , ...,  0.04696646,\n",
              "         -0.06039643, -0.00335863],\n",
              "        [ 0.03886195, -0.0283124 , -0.02234175, ...,  0.00904629,\n",
              "         -0.02847458, -0.00952092]], shape=(3, 384), dtype=float32))"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Student task: Write a semantically different prediction sentence and compute embeddings\n",
        "# Complete the sections with **********\n",
        "\n",
        "labels = [\"Cusco is in Peru\", \"Ayacucho is a region\", \"Trujillo beaches are marvelous\"]\n",
        "preds = [\n",
        "    \"Peru includes Cusco\",\n",
        "    \"Ayacucho is a department\",\n",
        "    # Write a sentence that is very semantically different from the prediction\n",
        "    # \"***********\"\n",
        "    \"The beaches in Trujillo are overcrowded and poorly maintained\"\n",
        "]\n",
        "\n",
        "\n",
        "# Get the embeddings for each sentence\n",
        "pred_embeddings = model.encode(preds)\n",
        "label_embeddings = model.encode(labels)\n",
        "\n",
        "\n",
        "assert pred_embeddings.shape == (3, 384), (\n",
        "    f\"Expected shape (3, 384), got {pred_embeddings.shape}\"\n",
        ")\n",
        "assert label_embeddings.shape == (3, 384), (\n",
        "    f\"Expected shape (3, 384), got {label_embeddings.shape}\"\n",
        ")\n",
        "\n",
        "pred_embeddings, label_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6e3b346c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pair 1:\n",
            "  Pred: Peru includes Cusco\n",
            "  Label: Cusco is in Peru\n",
            "  Cosine Similarity: 0.9358\n",
            "\n",
            "Pair 2:\n",
            "  Pred: Ayacucho is a department\n",
            "  Label: Ayacucho is a region\n",
            "  Cosine Similarity: 0.7663\n",
            "\n",
            "Pair 3:\n",
            "  Pred: The beaches in Trujillo are overcrowded and poorly maintained\n",
            "  Label: Trujillo beaches are marvelous\n",
            "  Cosine Similarity: 0.7471\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate the cosine similarity for each pair of embeddings\n",
        "# No changes needed in this cell, but if it fails, check the above cell\n",
        "\n",
        "cosine_similarity = [\n",
        "    # Cosine similarity for two vectors a and b is defined as:\n",
        "    # cos_sim(a, b) = (a . b) / (||a|| * ||b||)\n",
        "    # where (a . b) is the dot product of a and b,\n",
        "    # and ||a|| and ||b|| are the magnitudes (norms) of vectors a and b respectively.\n",
        "    float(\n",
        "        np.dot(pred_embeddings[i], label_embeddings[i])\n",
        "        / np.linalg.norm(pred_embeddings[i])\n",
        "        / np.linalg.norm(label_embeddings[i])\n",
        "    )\n",
        "    for i in range(len(preds))\n",
        "]\n",
        "\n",
        "# Compute cosine similarity between the two embeddings\n",
        "for i, (p, l, cos_sim) in enumerate(zip(preds, labels, cosine_similarity)):\n",
        "    print(f\"Pair {i + 1}:\")\n",
        "    print(f\"  Pred: {p}\")\n",
        "    print(f\"  Label: {l}\")\n",
        "    print(f\"  Cosine Similarity: {cos_sim:.4f}\\n\")\n",
        "\n",
        "# Check that the last pair has the lowest similarity\n",
        "assert cosine_similarity[-1] < cosine_similarity[0], (\n",
        "    \"The last pair should have the lowest cosine similarity. Please check your prediction sentence.\"\n",
        ")\n",
        "assert cosine_similarity[-1] < cosine_similarity[1], (\n",
        "    \"The last pair should have the lowest cosine similarity. Please check your prediction sentence.\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bbaf675",
      "metadata": {},
      "source": [
        "## Functional Correctness\n",
        "Let's evaluate code-generation by running a tiny function against unit tests (execution accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6ecc358d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Proportion of tests passed: 0.6666666666666666\n"
          ]
        }
      ],
      "source": [
        "# Student task: Complete the evaluation of the sort_and_normalize function\n",
        "# Complete the sections with **********\n",
        "\n",
        "\n",
        "def sort_and_normalize(s: str) -> str:\n",
        "    \"\"\"Sort the words in the string\"\"\"\n",
        "\n",
        "    # Our toy function will fail on this edge case\n",
        "    if \"armadillo\" in s:\n",
        "        s = s.replace(\"armadillo\", \"kitty\")\n",
        "\n",
        "    return \" \".join(sorted(s.split()))\n",
        "\n",
        "\n",
        "preds = [\n",
        "    \"the capybara is the largest rodent\",\n",
        "    \"an armadillo has a hard shell\",\n",
        "    \"elephants are the largest land animals\",\n",
        "]\n",
        "labels = [\n",
        "    \"capybara is largest rodent the the\",\n",
        "    \"a an armadillo hard has shell\",\n",
        "    \"animals are elephants land largest the\",\n",
        "]\n",
        "\n",
        "# Write tests to check if sort_and_normalize works correctly\n",
        "results = [\n",
        "    sort_and_normalize(p) == l\n",
        "    for p, l in zip(preds, labels)\n",
        "]\n",
        "\n",
        "print(\"Proportion of tests passed:\", sum(results) / len(results))\n",
        "\n",
        "assert sum(results) == 2, (\n",
        "    f\"2 tests should pass, but got {sum(results)}. Please check how your are evaluating the results.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d5b5ab6",
      "metadata": {},
      "source": [
        "## Pass@k\n",
        "\n",
        "Let's simulate multiple samples for a single task and compute pass@k (1 if any sample equals the gold)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "e5459f06",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pass@4 = 1\n"
          ]
        }
      ],
      "source": [
        "# Student task: Implement pass_at_k\n",
        "# Complete the sections with **********\n",
        "\n",
        "label = \"Lima\"\n",
        "samples = [\"Lima\", \"Arequipa\", \"Cusco\", \"Lima\"]\n",
        "\n",
        "\n",
        "# Implement pass_at_k with signature (samples: List[str], label: str) -> int\n",
        "# **********\n",
        "def pass_at_k(Samples: list[str], label: str) -> int:\n",
        "    \"\"\"Return 1 if label is in the top k samples, else 0\"\"\"\n",
        "    return int(any(i == label for i in Samples ))\n",
        "\n",
        "\n",
        "\n",
        "print(\"pass@4 =\", pass_at_k(samples, label))\n",
        "\n",
        "assert pass_at_k(samples, label) == 1, (\n",
        "    f\"pass@4 should be 1, but got {pass_at_k(samples, label)}. Please check your pass_at_k function.\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59b336e1",
      "metadata": {},
      "source": [
        "## LLM as a Judge\n",
        "\n",
        "Let's create a function that calls an LLM to compare predicted values and reference values (if applicable) and return a score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "97697d6b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n"
          ]
        },
        {
          "ename": "RateLimitError",
          "evalue": "litellm.RateLimitError: RateLimitError: OpenAIException - You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "File \u001b[0;32m~/dev/Spam_Classifier/.venv/lib/python3.10/site-packages/litellm/llms/openai/openai.py:762\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[0;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params, shared_session)\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 762\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OpenAIError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/dev/Spam_Classifier/.venv/lib/python3.10/site-packages/litellm/llms/openai/openai.py:690\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[0;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params, shared_session)\u001b[0m\n\u001b[1;32m    676\u001b[0m logging_obj\u001b[38;5;241m.\u001b[39mpre_call(\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m    678\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mopenai_client\u001b[38;5;241m.\u001b[39mapi_key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    684\u001b[0m     },\n\u001b[1;32m    685\u001b[0m )\n\u001b[1;32m    687\u001b[0m (\n\u001b[1;32m    688\u001b[0m     headers,\n\u001b[1;32m    689\u001b[0m     response,\n\u001b[0;32m--> 690\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_sync_openai_chat_completion_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopenai_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopenai_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    697\u001b[0m logging_obj\u001b[38;5;241m.\u001b[39mmodel_call_details[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m headers\n",
            "File \u001b[0;32m~/dev/Spam_Classifier/.venv/lib/python3.10/site-packages/litellm/litellm_core_utils/logging_utils.py:237\u001b[0m, in \u001b[0;36mtrack_llm_api_timing.<locals>.decorator.<locals>.sync_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 237\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m~/dev/Spam_Classifier/.venv/lib/python3.10/site-packages/litellm/llms/openai/openai.py:502\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.make_sync_openai_chat_completion_request\u001b[0;34m(self, openai_client, data, timeout, logging_obj)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "File \u001b[0;32m~/dev/Spam_Classifier/.venv/lib/python3.10/site-packages/litellm/llms/openai/openai.py:477\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.make_sync_openai_chat_completion_request\u001b[0;34m(self, openai_client, data, timeout, logging_obj)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 477\u001b[0m     raw_response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_raw_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m~/dev/Spam_Classifier/.venv/lib/python3.10/site-packages/openai/_legacy_response.py:364\u001b[0m, in \u001b[0;36mto_raw_response_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extra_headers\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m~/dev/Spam_Classifier/.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:286\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/dev/Spam_Classifier/.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:1192\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1191\u001b[0m validate_response_format(response_format)\n\u001b[0;32m-> 1192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_retention\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msafety_identifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mverbosity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/dev/Spam_Classifier/.venv/lib/python3.10/site-packages/openai/_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1256\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1257\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1258\u001b[0m )\n\u001b[0;32m-> 1259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m~/dev/Spam_Classifier/.venv/lib/python3.10/site-packages/openai/_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1046\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1047\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
            "File \u001b[0;32m~/dev/Spam_Classifier/.venv/lib/python3.10/site-packages/litellm/main.py:2369\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, verbosity, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\u001b[0m\n\u001b[1;32m   2363\u001b[0m     logging\u001b[38;5;241m.\u001b[39mpost_call(\n\u001b[1;32m   2364\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m   2365\u001b[0m         api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[1;32m   2366\u001b[0m         original_response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m   2367\u001b[0m         additional_args\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: headers},\n\u001b[1;32m   2368\u001b[0m     )\n\u001b[0;32m-> 2369\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   2371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optional_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   2372\u001b[0m     \u001b[38;5;66;03m## LOGGING\u001b[39;00m\n",
            "File \u001b[0;32m~/dev/Spam_Classifier/.venv/lib/python3.10/site-packages/litellm/main.py:2341\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, verbosity, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\u001b[0m\n\u001b[1;32m   2340\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2341\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai_chat_completions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2342\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2343\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2344\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2345\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2346\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2347\u001b[0m \u001b[43m            \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2348\u001b[0m \u001b[43m            \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2349\u001b[0m \u001b[43m            \u001b[49m\u001b[43macompletion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2350\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2351\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2352\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2353\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2354\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m   2355\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2356\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pass AsyncOpenAI, OpenAI client\u001b[39;49;00m\n\u001b[1;32m   2357\u001b[0m \u001b[43m            \u001b[49m\u001b[43morganization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morganization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2358\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2359\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshared_session\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshared_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2360\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2361\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2362\u001b[0m     \u001b[38;5;66;03m## LOGGING - log the original exception returned\u001b[39;00m\n",
            "File \u001b[0;32m~/dev/Spam_Classifier/.venv/lib/python3.10/site-packages/litellm/llms/openai/openai.py:773\u001b[0m, in \u001b[0;36mOpenAIChatCompletion.completion\u001b[0;34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params, shared_session)\u001b[0m\n\u001b[1;32m    772\u001b[0m     error_headers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(error_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 773\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    774\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mstatus_code,\n\u001b[1;32m    775\u001b[0m     message\u001b[38;5;241m=\u001b[39merror_text,\n\u001b[1;32m    776\u001b[0m     headers\u001b[38;5;241m=\u001b[39merror_headers,\n\u001b[1;32m    777\u001b[0m     body\u001b[38;5;241m=\u001b[39merror_body,\n\u001b[1;32m    778\u001b[0m )\n",
            "\u001b[0;31mOpenAIError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 64\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Write a rubric for evaluating if the prediction is the capital of the label country\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# 1.0 if correct, 0.5 if a city in the same country, 0.0 otherwise\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# **********\u001b[39;00m\n\u001b[1;32m     57\u001b[0m RUBRIC \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are to evaluate if the predicted city is the capital of the given country.\u001b[39m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124m- If the predicted city is the capital of the country, give a score of 1.0.\u001b[39m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124m- If the predicted city is a city in the country but not the capital, give a score of 0.5.\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124m- If the predicted city is not in the country, give a score of 0.0.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mllm_as_judge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mManila\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPhilippines\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrubric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRUBRIC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     70\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mManila is the capital of the Philippines\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m     73\u001b[0m     llm_as_judge(\n\u001b[1;32m     74\u001b[0m         pred\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCebu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m     79\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCebu is a city in the Philippines, but not the capital\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m     82\u001b[0m     llm_as_judge(\n\u001b[1;32m     83\u001b[0m         pred\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokyo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     88\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokyo is not in the Philippines\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "Cell \u001b[0;32mIn[16], line 32\u001b[0m, in \u001b[0;36mllm_as_judge\u001b[0;34m(pred, rubric, label)\u001b[0m\n\u001b[1;32m     26\u001b[0m     USER_PROMPT \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Here is the label: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Call the LLM using litellm with the system and user prompts (use the model gpt-5-nano)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# See: https://github.com/BerriAI/litellm\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# response = completion(**********)\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-5-nano\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mSYSTEM_PROMPT\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mUSER_PROMPT\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m text_response \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM response:\u001b[39m\u001b[38;5;124m\"\u001b[39m, text_response)\n",
            "File \u001b[0;32m~/dev/Spam_Classifier/.venv/lib/python3.10/site-packages/litellm/utils.py:1405\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[1;32m   1402\u001b[0m     logging_obj\u001b[38;5;241m.\u001b[39mfailure_handler(\n\u001b[1;32m   1403\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[1;32m   1404\u001b[0m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[0;32m-> 1405\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "File \u001b[0;32m~/dev/Spam_Classifier/.venv/lib/python3.10/site-packages/litellm/utils.py:1274\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1272\u001b[0m         print_verbose(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[0;32m-> 1274\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1275\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_streaming_request(\n\u001b[1;32m   1277\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   1278\u001b[0m     call_type\u001b[38;5;241m=\u001b[39mcall_type,\n\u001b[1;32m   1279\u001b[0m ):\n",
            "File \u001b[0;32m~/dev/Spam_Classifier/.venv/lib/python3.10/site-packages/litellm/main.py:4080\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, verbosity, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\u001b[0m\n\u001b[1;32m   4077\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m   4078\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   4079\u001b[0m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[0;32m-> 4080\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4081\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4082\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4083\u001b[0m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4084\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4085\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4086\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/dev/Spam_Classifier/.venv/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2340\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   2338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[1;32m   2339\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, litellm_response_headers)\n\u001b[0;32m-> 2340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   2341\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2342\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mLITELLM_EXCEPTION_TYPES:\n",
            "File \u001b[0;32m~/dev/Spam_Classifier/.venv/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:367\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ExceptionCheckers\u001b[38;5;241m.\u001b[39mis_error_str_rate_limit(error_str):\n\u001b[1;32m    366\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RateLimitError(\n\u001b[1;32m    368\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRateLimitError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    369\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    370\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[1;32m    371\u001b[0m         response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    372\u001b[0m     )\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ExceptionCheckers\u001b[38;5;241m.\u001b[39mis_error_str_context_window_exceeded(error_str):\n\u001b[1;32m    374\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: litellm.RateLimitError: RateLimitError: OpenAIException - You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors."
          ]
        }
      ],
      "source": [
        "# Student task: Complete the LLM-as-a-judge function\n",
        "# Complete the sections with **********\n",
        "\n",
        "\n",
        "def llm_as_judge(pred: str, rubric: str, label: str | None = None) -> float:\n",
        "    \"\"\"Use an LLM to judge the quality of a prediction against a rubric and optional label.\"\"\"\n",
        "    from litellm import completion\n",
        "\n",
        "    # Write a system prompt that instructs the LLM to use the rubric to score the prediction\n",
        "    # The response should be formatted as:\n",
        "    # <reasoning>...</reasoning>\n",
        "    # <score>FLOAT_ANSWER</score>\n",
        "    # where FLOAT_ANSWER is a float between 0 and 1.\n",
        "    # We will extract FLOAT_ANSWER from the response later\n",
        "\n",
        "    SYSTEM_PROMPT = f\"\"\"You are an expert evalutor. use the rubric to score the prediction. \n",
        "    Respond with <reasoning>...</reasoning> and \n",
        "    <score>FLOAT_ANSWER</score> where FLOAT_ANSWER is a float between 0 and 1.\n",
        "    Rubric: {rubric}\"\"\"\n",
        "\n",
        "\n",
        "    # Create a user prompt with the prediction and, optionally, the label\n",
        "    # **********\n",
        "    USER_PROMPT = f\"\"\"Here is the prediction: {pred}.\"\"\"\n",
        "    if label is not None:\n",
        "        USER_PROMPT += f\" Here is the label: {label}.\"\n",
        "\n",
        "\n",
        "    # Call the LLM using litellm with the system and user prompts (use the model gpt-5-nano)\n",
        "    # See: https://github.com/BerriAI/litellm\n",
        "    # response = completion(**********)\n",
        "    response = completion( model= \"gpt-5-nano\", messages= [\n",
        "        {\"role\":\"system\",\"content\":SYSTEM_PROMPT},{\"role\":\"user\",\"content\":USER_PROMPT}\n",
        "        ])\n",
        "\n",
        "\n",
        "    text_response = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "    print(\"LLM response:\", text_response)\n",
        "\n",
        "    # Extract FLOAT_ANSWER from the response\n",
        "\n",
        "    # float_answer = **********\n",
        "    print(text_response)\n",
        "    print(\"*********\")\n",
        "    print(text_response.split(\"<score>\")[1])\n",
        "\n",
        "    float_answer = float(text_response.split(\"<score>\")[1].split(\"<score>\")[0].strip())\n",
        "\n",
        "\n",
        "    return float_answer\n",
        "\n",
        "\n",
        "# Write a rubric for evaluating if the prediction is the capital of the label country\n",
        "# 1.0 if correct, 0.5 if a city in the same country, 0.0 otherwise\n",
        "\n",
        "# **********\n",
        "RUBRIC = \"\"\"You are to evaluate if the predicted city is the capital of the given country.\n",
        "- If the predicted city is the capital of the country, give a score of 1.0.\n",
        "- If the predicted city is a city in the country but not the capital, give a score of 0.5.\n",
        "- If the predicted city is not in the country, give a score of 0.0.\"\"\"\n",
        "\n",
        "\n",
        "assert (\n",
        "    llm_as_judge(\n",
        "        pred=\"Manila\",\n",
        "        label=\"Philippines\",\n",
        "        rubric=RUBRIC,\n",
        "    )\n",
        "    == 1.0\n",
        "), \"Manila is the capital of the Philippines\"\n",
        "\n",
        "assert (\n",
        "    llm_as_judge(\n",
        "        pred=\"Cebu\",\n",
        "        label=\"Philippines\",\n",
        "        rubric=RUBRIC,\n",
        "    )\n",
        "    == 0.5\n",
        "), \"Cebu is a city in the Philippines, but not the capital\"\n",
        "\n",
        "assert (\n",
        "    llm_as_judge(\n",
        "        pred=\"Tokyo\",\n",
        "        label=\"Philippines\",\n",
        "        rubric=RUBRIC,\n",
        "    )\n",
        "    == 0.0\n",
        "), \"Tokyo is not in the Philippines\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa792f32",
      "metadata": {},
      "source": [
        "Congrats! You have completed the evaluation exercise. Proper evaluation is the bedrock for building reliable AI systems. Great job! "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4089a86",
      "metadata": {},
      "source": [
        "<br /><br /><br /><br /><br /><br /><br /><br /><br />"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
